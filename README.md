# Maya: Multimodal Multilingual LLM

- Models and Dataset at [HuggingFace](https://huggingface.co/maya-multimodal)
- Paper: 

## Install

Follow the installation process of LLaVA below. Then do this

```
pip install chardet==5.2.0
pip install datasets==2.15.0
pip install deepspeed==0.14.2
pip install fastapi==0.111.0
pip install transformers==4.42.3
pip install accelerate==0.27.2
```
## Contributors
- Team Leads: Nahid, Karthik, Surya
- Satya https://github.com/Satyajitv
- Iftekhar Uddin https://github.com/iuddin
- Drishti Sushma  https://github.com/DrishtiShrrrma
- Roshan Santhosh https://github.com/rsk2327
- Cecilia Liu, Snegha, Shayakh, Anthony, Isha
- Ryan Chan https://github.com/rchan26
- Sangyeon Kim https://github.com/KimSangYeon-DGU
- Snehanshu https://github.com/pilot-j



## Contents
- [Install](#install)
- [LLaVA Weights](#llava-weights)
- [Demo](#Demo)
- [Model Zoo](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md)
- [Dataset](https://github.com/haotian-liu/LLaVA/blob/main/docs/Data.md)
- [Train](#train)
- [Evaluation](#evaluation)

## Install

## Model Weights and Inference

## Training

## Evaluation

In LLaVA-1.5, we evaluate models on a diverse set of 12 benchmarks. To ensure the reproducibility, we evaluate the models with greedy decoding. We do not evaluate using beam search to make the inference process consistent with the chat demo of real-time outputs.

See [Evaluation.md](https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md).

### GPT-assisted Evaluation

Our GPT-assisted evaluation pipeline for multimodal modeling is provided for a comprehensive understanding of the capabilities of vision-language models.  Please see our paper for more details.

1. Generate LLaVA responses

```Shell
python model_vqa.py \
    --model-path ./checkpoints/LLaVA-13B-v0 \
    --question-file \
    playground/data/coco2014_val_qa_eval/qa90_questions.jsonl \
    --image-folder \
    /path/to/coco2014_val \
    --answers-file \
    /path/to/answer-file-our.jsonl
```

2. Evaluate the generated responses.  In our case, [`answer-file-ref.jsonl`](./playground/data/coco2014_val_qa_eval/qa90_gpt4_answer.jsonl) is the response generated by text-only GPT-4 (0314), with the context captions/boxes provided.

```Shell
OPENAI_API_KEY="sk-***********************************" python llava/eval/eval_gpt_review_visual.py \
    --question playground/data/coco2014_val_qa_eval/qa90_questions.jsonl \
    --context llava/eval/table/caps_boxes_coco2014_val_80.jsonl \
    --answer-list \
    /path/to/answer-file-ref.jsonl \
    /path/to/answer-file-our.jsonl \
    --rule llava/eval/table/rule.json \
    --output /path/to/review.json
```

3. Summarize the evaluation results

```Shell
python summarize_gpt_review.py
```

## Citation

If you find Maya useful for your research and applications, please cite using this BibTeX:


## Acknowledgement

- This project would not be possible without the support of Cohere and their Aya-35B API grant. We are thankful to Sara Hooker, Madeline, Shivalika, Shristhi and the entire Cohere for AI team for
- We thank Pytho for their generaous GPU grant 
- This codebase is based on [LLaVA](https://github.com/haotian-liu/LLaVA) Thank you for the easily understable codebase. 



